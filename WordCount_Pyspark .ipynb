{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff1876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(text=\"In a world full of #technology, understanding data is key to success. Data analytics, 'AI', and machine learning are transforming industries. Companies are racing to harness the power of data-driven insights; But,  data is messy and comes in various formats - structured, unstructured, and semi-structured. The challenge is to clean, process, and  analyze this data effectively. There's a growing demand for data scientists, analysts, and engineers who can unlock the value hidden within the data.\")\n",
      "+-------------+--------+\n",
      "|words        |count(1)|\n",
      "+-------------+--------+\n",
      "|a            |2       |\n",
      "|ai           |1       |\n",
      "|analysts     |1       |\n",
      "|analytics    |1       |\n",
      "|analyze      |1       |\n",
      "|and          |5       |\n",
      "|are          |2       |\n",
      "|but          |1       |\n",
      "|can          |1       |\n",
      "|challenge    |1       |\n",
      "|clean        |1       |\n",
      "|comes        |1       |\n",
      "|companies    |1       |\n",
      "|data         |7       |\n",
      "|demand       |1       |\n",
      "|driven       |1       |\n",
      "|effectively  |1       |\n",
      "|engineers    |1       |\n",
      "|for          |1       |\n",
      "|formats      |1       |\n",
      "|full         |1       |\n",
      "|growing      |1       |\n",
      "|harness      |1       |\n",
      "|hidden       |1       |\n",
      "|in           |2       |\n",
      "|industries   |1       |\n",
      "|insights     |1       |\n",
      "|is           |3       |\n",
      "|key          |1       |\n",
      "|learning     |1       |\n",
      "|machine      |1       |\n",
      "|messy        |1       |\n",
      "|of           |2       |\n",
      "|power        |1       |\n",
      "|process      |1       |\n",
      "|racing       |1       |\n",
      "|s            |1       |\n",
      "|scientists   |1       |\n",
      "|semi         |1       |\n",
      "|structured   |2       |\n",
      "|success      |1       |\n",
      "|technology   |1       |\n",
      "|the          |4       |\n",
      "|there        |1       |\n",
      "|this         |1       |\n",
      "|to           |3       |\n",
      "|transforming |1       |\n",
      "|understanding|1       |\n",
      "|unlock       |1       |\n",
      "|unstructured |1       |\n",
      "|value        |1       |\n",
      "|various      |1       |\n",
      "|who          |1       |\n",
      "|within       |1       |\n",
      "|world        |1       |\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, split, explode, count, lower\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "class WordCount:\n",
    "    def createData(self):\n",
    "        line = \"\"\"In a world full of #technology, understanding data is key to success. Data analytics, 'AI', and machine learning are transforming industries. Companies are racing to harness the power of data-driven insights; But,  data is messy and comes in various formats - structured, unstructured, and semi-structured. The challenge is to clean, process, and  analyze this data effectively. There's a growing demand for data scientists, analysts, and engineers who can unlock the value hidden within the data.\"\"\"\n",
    "\n",
    "        dataDf = spark.createDataFrame([(line,)], ['text'])\n",
    "        return dataDf\n",
    "\n",
    "    def wCount(self, inputDf):\n",
    "        inputDf.createOrReplaceTempView('table')\n",
    "        query = \"\"\"with cte as (\n",
    "                select regexp_replace(text, \"[';.,#*-]\", ' ') as text\n",
    "                from table),\n",
    "                M1 as (select \n",
    "                explode(split(lower(text), ' ')) as words \n",
    "                from cte)\n",
    "                select \n",
    "                words, count(1)\n",
    "                from M1 \n",
    "                where words != '' \n",
    "                group by words \n",
    "                order by words\"\"\"\n",
    "        return spark.sql(query)\n",
    "\n",
    "    def wordCount(self, inputDf):\n",
    "        pattern = \"[';.,#*-_]\"\n",
    "        replacement = ' '\n",
    "        regexDf = inputDf.select(regexp_replace(col('text'), pattern, replacement).alias('words'))\n",
    "        splitDf = regexDf.select(explode(split(lower('words'), ' ')).alias('words'))\n",
    "        countDf = splitDf.groupBy('words').agg(count('*').alias('counts'))\\\n",
    "                        .filter(col('words') != '').orderBy('words')\n",
    "        return countDf\n",
    "\n",
    "\n",
    "ob = WordCount()\n",
    "inputDf = ob.createData()\n",
    "print(inputDf.head())\n",
    "\n",
    "resultDf = ob.wCount(inputDf)\n",
    "resultDf.show(100, False)\n",
    "\n",
    "#resultDf = ob.wordCount(inputDf)\n",
    "#resultDf.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0fbc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|      words|counts|\n",
      "+-----------+------+\n",
      "|          a|     2|\n",
      "|         ai|     1|\n",
      "|   analysts|     1|\n",
      "|  analytics|     1|\n",
      "|    analyze|     1|\n",
      "|        and|     5|\n",
      "|        are|     2|\n",
      "|        but|     1|\n",
      "|        can|     1|\n",
      "|  challenge|     1|\n",
      "|      clean|     1|\n",
      "|      comes|     1|\n",
      "|  companies|     1|\n",
      "|       data|     7|\n",
      "|     demand|     1|\n",
      "|     driven|     1|\n",
      "|effectively|     1|\n",
      "|  engineers|     1|\n",
      "|        for|     1|\n",
      "|    formats|     1|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-----------+------+\n",
      "|      words|counts|\n",
      "+-----------+------+\n",
      "|          a|     2|\n",
      "|         ai|     1|\n",
      "|   analysts|     1|\n",
      "|  analytics|     1|\n",
      "|    analyze|     1|\n",
      "|        and|     5|\n",
      "|        are|     2|\n",
      "|        but|     1|\n",
      "|        can|     1|\n",
      "|  challenge|     1|\n",
      "|      clean|     1|\n",
      "|      comes|     1|\n",
      "|  companies|     1|\n",
      "|       data|     7|\n",
      "|     demand|     1|\n",
      "|     driven|     1|\n",
      "|effectively|     1|\n",
      "|  engineers|     1|\n",
      "|        for|     1|\n",
      "|    formats|     1|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, lower, split, explode\n",
    "\n",
    "os.environ['PYSPARK'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "        \n",
    "class WordCount:\n",
    "    \n",
    "    def createData(self):\n",
    "        line = \"\"\"In a world full of #technology, understanding data is key to success. Data analytics, 'AI', and machine learning are transforming industries. Companies are racing to harness the power of data-driven insights; But,  data is messy and comes in various formats - structured, unstructured, and semi-structured. The challenge is to clean, process, and  analyze this data effectively. There's a growing demand for data scientists, analysts, and engineers who can unlock the value hidden within the data.\"\"\"\n",
    "        df = spark.createDataFrame([(line,)], ['text'])\n",
    "        return df\n",
    "    \n",
    "    def wordcountSQL(self, inputdf):\n",
    "        inputDf.createOrReplaceTempView('table')\n",
    "        query = \"\"\"\n",
    "        with cte as (\n",
    "                select regexp_replace(text, \"[';.,#*-]\", ' ') as text\n",
    "                from table),\n",
    "                \n",
    "                M1 as (select \n",
    "                explode(split(lower(text), ' ')) as words \n",
    "                from cte)\n",
    "                \n",
    "                select \n",
    "                words, count(1)\n",
    "                from M1 \n",
    "                where words != '' \n",
    "                group by words \n",
    "                order by words    \n",
    "        \"\"\"\n",
    "        return spark.sql(query)\n",
    "    \n",
    "    def wordCountPyspark(self, inputdf):\n",
    "        pattern = \"[';.,#*-]\"\n",
    "        replacement = ' '\n",
    "        regexDf = inputdf.select(regexp_replace(col('text'), pattern, replacement).alias('words'))\n",
    "        splitDf = regexDf.select(explode(split(lower('words'), ' ')).alias('words'))\n",
    "        countDf = splitDf.groupBy('words').agg(count('*').alias('counts')).filter(col('words')!='').orderBy('words')\n",
    "        print(countDf.show())\n",
    "        return countDf\n",
    "         \n",
    "    \n",
    "w1 = WordCount()\n",
    "inputdf = w1.createData()\n",
    "#print(inputdf.head())\n",
    "#inputdf.show()\n",
    "#print(\"**********\")\n",
    "#resultdf = w1.wordcountSQL(inputdf)\n",
    "#resultdf.show(200)\n",
    "redultDf = w1.wordCountPyspark(inputdf)\n",
    "redultDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70f820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c2e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
